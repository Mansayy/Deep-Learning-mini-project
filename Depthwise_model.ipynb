{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9Nw2rbW8fe8",
        "outputId": "e2272e0d-a66a-4c23-c0f4-60fde400eb8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from torchviz) (2.0.0+cu118)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.9/dist-packages (from torchviz) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->torchviz) (3.11.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->torchviz) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->torchviz) (3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->torchviz) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->torchviz) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->torchviz) (2.0.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->torchviz) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->torchviz) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->torchviz) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->torchviz) (1.3.0)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4147 sha256=fcf5b10069c6c223b7c585c5c8575efa5f3a8d5dc1df9399e0ff586703de10b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/65/6e/db2515eb1dc760fecd36b40d54df65c1e18534013f1c037e2e\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install torchviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "feSK6nIy92M_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchviz\n",
        "import torchsummary\n",
        "from torchvision.models import resnet18\n",
        "import torchvision.transforms as transforms\n",
        "from torchviz import make_dot\n",
        "\n",
        "from torchvision.models.resnet import ResNet18_Weights\n",
        "\n",
        "import os\n",
        "import argparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "e-HYNWkM94_n"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwGGTjel98m-",
        "outputId": "aaf4ca04-67ca-4599-d5e1-b2932563d0b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n"
          ]
        }
      ],
      "source": [
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvreGtxU-CpG",
        "outputId": "0d9f7e75-112f-4c56-9bd9-af1b07e23697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 83030149.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=100, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eU2-NVrFDJTh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary\n",
        "\n",
        "class DepthwiseSeparableConv2d(nn.Module):\n",
        "    \"\"\"Depthwise separable convolution 2d.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super().__init__()\n",
        "        self.depthwise_conv = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=in_channels, bias=False)\n",
        "        self.pointwise_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.depthwise_conv(x)\n",
        "        out = self.pointwise_conv(out)\n",
        "        out = self.bn(out)\n",
        "        out = nn.functional.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    \"\"\"Helper class to create a ResNet block.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1, use_depthwise=True):\n",
        "        super().__init__()\n",
        "        if use_depthwise:\n",
        "            self.conv1 = DepthwiseSeparableConv2d(in_channels, out_channels, stride=stride)\n",
        "            self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "            self.conv2 = DepthwiseSeparableConv2d(out_channels, out_channels, stride=1)\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "            self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "            self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        #self.bn= nn.BatchNorm2d(out_channels)\n",
        "        #self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            if use_depthwise:\n",
        "                self.shortcut = nn.Sequential(\n",
        "                    DepthwiseSeparableConv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0),\n",
        "                    nn.BatchNorm2d(out_channels)\n",
        "                )\n",
        "            else:\n",
        "                self.shortcut = nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                    nn.BatchNorm2d(out_channels)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        #out=nn.functional.batch_norm(out)\n",
        "        out = self.bn2(out)\n",
        "        out = nn.functional.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        #out=nn.functional.batch_norm(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.shortcut(x)\n",
        "        out = nn.functional.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ModifiedResNet18(nn.Module):\n",
        "    \"\"\"ResNet-18 model for CIFAR-10.\"\"\"\n",
        "    def __init__(self, num_classes=10, use_depthwise=True):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.layer1 = nn.Sequential(\n",
        "            ResNetBlock(32, 64, stride=1,use_depthwise=use_depthwise),\n",
        "            ResNetBlock(64, 64, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(64, 64,use_depthwise=use_depthwise),\n",
        "            ResNetBlock(64, 64, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(64, 64, use_depthwise=use_depthwise)\n",
        "            \n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            ResNetBlock(64, 128, stride=2,use_depthwise=use_depthwise),\n",
        "            ResNetBlock(128, 128, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(128, 128, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(128, 128,use_depthwise=use_depthwise),\n",
        "            ResNetBlock(128, 128, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(128, 128, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(128, 128, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(128, 128, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(128, 128, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(128, 128, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(128, 128, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(128, 128, use_depthwise=use_depthwise)\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            ResNetBlock(128, 256, stride=2,use_depthwise=use_depthwise),\n",
        "            ResNetBlock(256, 256, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(256, 256, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(256, 256,use_depthwise=use_depthwise),\n",
        "            ResNetBlock(256, 256, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(256, 256, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(256, 256, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(256, 256, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(256, 256, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(256, 256, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(256, 256, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(256, 256, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(256, 256, use_depthwise=use_depthwise)\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            ResNetBlock(256, 512, stride=1,use_depthwise=use_depthwise),\n",
        "            ResNetBlock(512, 512, use_depthwise=use_depthwise),\n",
        "            ResNetBlock(512, 512,use_depthwise=use_depthwise),\n",
        "            ResNetBlock(512, 512,use_depthwise=use_depthwise),\n",
        "            ResNetBlock(512, 512,use_depthwise=use_depthwise)\n",
        "            \n",
        "            \n",
        "        )\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "        self.Dropout = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(512, num_classes)\n",
        "        \n",
        "       # self.fc2 = nn.Linear(1024, num_classes)\n",
        "        #self.fc = nn.Linear(512, num_classes)\n",
        "        #self.fc = nn.Linear(512, num_classes)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "       #out=nn.functional.batch_norm(out)\n",
        "        out = self.bn1(out)\n",
        "        out = nn.functional.relu(out)\n",
        "        out = self.maxpool(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = self.Dropout(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc1(out)\n",
        "        #out = self.functional.softmax(out)\n",
        "        #out = self.fc2(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1ZgK0Db1-QLV"
      },
      "outputs": [],
      "source": [
        "model = ModifiedResNet18()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc3rb4f4-Xql",
        "outputId": "c5532393-2924-433e-fb24-385c38f9ace4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4906794\n"
          ]
        }
      ],
      "source": [
        "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(num_trainable_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7ra-jSn-bY0",
        "outputId": "74fc2edf-edb0-40de-969a-1c0fffe50d82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]           4,704\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "         MaxPool2d-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          18,432\n",
            "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
            "            Conv2d-6           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-7           [-1, 64, 32, 32]             128\n",
            "            Conv2d-8           [-1, 64, 32, 32]           2,048\n",
            "       BatchNorm2d-9           [-1, 64, 32, 32]             128\n",
            "      ResNetBlock-10           [-1, 64, 32, 32]               0\n",
            "           Conv2d-11           [-1, 64, 32, 32]             576\n",
            "           Conv2d-12           [-1, 64, 32, 32]           4,096\n",
            "      BatchNorm2d-13           [-1, 64, 32, 32]             128\n",
            "DepthwiseSeparableConv2d-14           [-1, 64, 32, 32]               0\n",
            "      BatchNorm2d-15           [-1, 64, 32, 32]             128\n",
            "           Conv2d-16           [-1, 64, 32, 32]             576\n",
            "           Conv2d-17           [-1, 64, 32, 32]           4,096\n",
            "      BatchNorm2d-18           [-1, 64, 32, 32]             128\n",
            "DepthwiseSeparableConv2d-19           [-1, 64, 32, 32]               0\n",
            "      BatchNorm2d-20           [-1, 64, 32, 32]             128\n",
            "      ResNetBlock-21           [-1, 64, 32, 32]               0\n",
            "           Conv2d-22          [-1, 128, 16, 16]          73,728\n",
            "      BatchNorm2d-23          [-1, 128, 16, 16]             256\n",
            "           Conv2d-24          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-25          [-1, 128, 16, 16]             256\n",
            "           Conv2d-26          [-1, 128, 16, 16]           8,192\n",
            "      BatchNorm2d-27          [-1, 128, 16, 16]             256\n",
            "      ResNetBlock-28          [-1, 128, 16, 16]               0\n",
            "           Conv2d-29          [-1, 128, 16, 16]           1,152\n",
            "           Conv2d-30          [-1, 128, 16, 16]          16,384\n",
            "      BatchNorm2d-31          [-1, 128, 16, 16]             256\n",
            "DepthwiseSeparableConv2d-32          [-1, 128, 16, 16]               0\n",
            "      BatchNorm2d-33          [-1, 128, 16, 16]             256\n",
            "           Conv2d-34          [-1, 128, 16, 16]           1,152\n",
            "           Conv2d-35          [-1, 128, 16, 16]          16,384\n",
            "      BatchNorm2d-36          [-1, 128, 16, 16]             256\n",
            "DepthwiseSeparableConv2d-37          [-1, 128, 16, 16]               0\n",
            "      BatchNorm2d-38          [-1, 128, 16, 16]             256\n",
            "      ResNetBlock-39          [-1, 128, 16, 16]               0\n",
            "           Conv2d-40          [-1, 128, 16, 16]           1,152\n",
            "           Conv2d-41          [-1, 128, 16, 16]          16,384\n",
            "      BatchNorm2d-42          [-1, 128, 16, 16]             256\n",
            "DepthwiseSeparableConv2d-43          [-1, 128, 16, 16]               0\n",
            "      BatchNorm2d-44          [-1, 128, 16, 16]             256\n",
            "           Conv2d-45          [-1, 128, 16, 16]           1,152\n",
            "           Conv2d-46          [-1, 128, 16, 16]          16,384\n",
            "      BatchNorm2d-47          [-1, 128, 16, 16]             256\n",
            "DepthwiseSeparableConv2d-48          [-1, 128, 16, 16]               0\n",
            "      BatchNorm2d-49          [-1, 128, 16, 16]             256\n",
            "      ResNetBlock-50          [-1, 128, 16, 16]               0\n",
            "           Conv2d-51            [-1, 256, 8, 8]         294,912\n",
            "      BatchNorm2d-52            [-1, 256, 8, 8]             512\n",
            "           Conv2d-53            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-54            [-1, 256, 8, 8]             512\n",
            "           Conv2d-55            [-1, 256, 8, 8]          32,768\n",
            "      BatchNorm2d-56            [-1, 256, 8, 8]             512\n",
            "      ResNetBlock-57            [-1, 256, 8, 8]               0\n",
            "           Conv2d-58            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-59            [-1, 256, 8, 8]             512\n",
            "           Conv2d-60            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-61            [-1, 256, 8, 8]             512\n",
            "      ResNetBlock-62            [-1, 256, 8, 8]               0\n",
            "           Conv2d-63            [-1, 256, 8, 8]           2,304\n",
            "           Conv2d-64            [-1, 256, 8, 8]          65,536\n",
            "      BatchNorm2d-65            [-1, 256, 8, 8]             512\n",
            "DepthwiseSeparableConv2d-66            [-1, 256, 8, 8]               0\n",
            "      BatchNorm2d-67            [-1, 256, 8, 8]             512\n",
            "           Conv2d-68            [-1, 256, 8, 8]           2,304\n",
            "           Conv2d-69            [-1, 256, 8, 8]          65,536\n",
            "      BatchNorm2d-70            [-1, 256, 8, 8]             512\n",
            "DepthwiseSeparableConv2d-71            [-1, 256, 8, 8]               0\n",
            "      BatchNorm2d-72            [-1, 256, 8, 8]             512\n",
            "      ResNetBlock-73            [-1, 256, 8, 8]               0\n",
            "           Conv2d-74            [-1, 256, 8, 8]           2,304\n",
            "           Conv2d-75            [-1, 256, 8, 8]          65,536\n",
            "      BatchNorm2d-76            [-1, 256, 8, 8]             512\n",
            "DepthwiseSeparableConv2d-77            [-1, 256, 8, 8]               0\n",
            "      BatchNorm2d-78            [-1, 256, 8, 8]             512\n",
            "           Conv2d-79            [-1, 256, 8, 8]           2,304\n",
            "           Conv2d-80            [-1, 256, 8, 8]          65,536\n",
            "      BatchNorm2d-81            [-1, 256, 8, 8]             512\n",
            "DepthwiseSeparableConv2d-82            [-1, 256, 8, 8]               0\n",
            "      BatchNorm2d-83            [-1, 256, 8, 8]             512\n",
            "      ResNetBlock-84            [-1, 256, 8, 8]               0\n",
            "           Conv2d-85            [-1, 256, 8, 8]           2,304\n",
            "           Conv2d-86            [-1, 512, 8, 8]         131,072\n",
            "      BatchNorm2d-87            [-1, 512, 8, 8]           1,024\n",
            "DepthwiseSeparableConv2d-88            [-1, 512, 8, 8]               0\n",
            "      BatchNorm2d-89            [-1, 512, 8, 8]           1,024\n",
            "           Conv2d-90            [-1, 512, 8, 8]           4,608\n",
            "           Conv2d-91            [-1, 512, 8, 8]         262,144\n",
            "      BatchNorm2d-92            [-1, 512, 8, 8]           1,024\n",
            "DepthwiseSeparableConv2d-93            [-1, 512, 8, 8]               0\n",
            "      BatchNorm2d-94            [-1, 512, 8, 8]           1,024\n",
            "           Conv2d-95            [-1, 256, 8, 8]             256\n",
            "           Conv2d-96            [-1, 512, 8, 8]         131,072\n",
            "      BatchNorm2d-97            [-1, 512, 8, 8]           1,024\n",
            "DepthwiseSeparableConv2d-98            [-1, 512, 8, 8]               0\n",
            "      BatchNorm2d-99            [-1, 512, 8, 8]           1,024\n",
            "     ResNetBlock-100            [-1, 512, 8, 8]               0\n",
            "          Conv2d-101            [-1, 512, 8, 8]           4,608\n",
            "          Conv2d-102            [-1, 512, 8, 8]         262,144\n",
            "     BatchNorm2d-103            [-1, 512, 8, 8]           1,024\n",
            "DepthwiseSeparableConv2d-104            [-1, 512, 8, 8]               0\n",
            "     BatchNorm2d-105            [-1, 512, 8, 8]           1,024\n",
            "          Conv2d-106            [-1, 512, 8, 8]           4,608\n",
            "          Conv2d-107            [-1, 512, 8, 8]         262,144\n",
            "     BatchNorm2d-108            [-1, 512, 8, 8]           1,024\n",
            "DepthwiseSeparableConv2d-109            [-1, 512, 8, 8]               0\n",
            "     BatchNorm2d-110            [-1, 512, 8, 8]           1,024\n",
            "     ResNetBlock-111            [-1, 512, 8, 8]               0\n",
            "          Conv2d-112            [-1, 512, 8, 8]           4,608\n",
            "          Conv2d-113            [-1, 512, 8, 8]         262,144\n",
            "     BatchNorm2d-114            [-1, 512, 8, 8]           1,024\n",
            "DepthwiseSeparableConv2d-115            [-1, 512, 8, 8]               0\n",
            "     BatchNorm2d-116            [-1, 512, 8, 8]           1,024\n",
            "          Conv2d-117            [-1, 512, 8, 8]           4,608\n",
            "          Conv2d-118            [-1, 512, 8, 8]         262,144\n",
            "     BatchNorm2d-119            [-1, 512, 8, 8]           1,024\n",
            "DepthwiseSeparableConv2d-120            [-1, 512, 8, 8]               0\n",
            "     BatchNorm2d-121            [-1, 512, 8, 8]           1,024\n",
            "     ResNetBlock-122            [-1, 512, 8, 8]               0\n",
            "          Conv2d-123            [-1, 512, 8, 8]           4,608\n",
            "          Conv2d-124            [-1, 512, 8, 8]         262,144\n",
            "     BatchNorm2d-125            [-1, 512, 8, 8]           1,024\n",
            "DepthwiseSeparableConv2d-126            [-1, 512, 8, 8]               0\n",
            "     BatchNorm2d-127            [-1, 512, 8, 8]           1,024\n",
            "          Conv2d-128            [-1, 512, 8, 8]           4,608\n",
            "          Conv2d-129            [-1, 512, 8, 8]         262,144\n",
            "     BatchNorm2d-130            [-1, 512, 8, 8]           1,024\n",
            "DepthwiseSeparableConv2d-131            [-1, 512, 8, 8]               0\n",
            "     BatchNorm2d-132            [-1, 512, 8, 8]           1,024\n",
            "     ResNetBlock-133            [-1, 512, 8, 8]               0\n",
            "AdaptiveAvgPool2d-134            [-1, 512, 1, 1]               0\n",
            "          Linear-135                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 4,905,386\n",
            "Trainable params: 4,905,386\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 33.25\n",
            "Params size (MB): 18.71\n",
            "Estimated Total Size (MB): 51.98\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(model.cuda(), (3, 32, 32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kM2sIx2V-lMk",
        "outputId": "aedaef73-175e-4651-b00a-57ec89c21ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[EPOCH: 1,   100] loss: 2.526\n",
            "[EPOCH: 1,   200] loss: 2.045\n",
            "[EPOCH: 1,   300] loss: 1.882\n",
            "[EPOCH: 1,   400] loss: 1.801\n",
            "[EPOCH: 1,   500] loss: 1.686\n",
            "Accuracy on test set: 44 %\n",
            "[EPOCH: 2,   100] loss: 1.507\n",
            "[EPOCH: 2,   200] loss: 1.483\n",
            "[EPOCH: 2,   300] loss: 1.414\n",
            "[EPOCH: 2,   400] loss: 1.281\n",
            "[EPOCH: 2,   500] loss: 1.219\n",
            "Accuracy on test set: 57 %\n",
            "[EPOCH: 3,   100] loss: 1.176\n",
            "[EPOCH: 3,   200] loss: 1.162\n",
            "[EPOCH: 3,   300] loss: 1.053\n",
            "[EPOCH: 3,   400] loss: 1.112\n",
            "[EPOCH: 3,   500] loss: 1.060\n",
            "Accuracy on test set: 64 %\n",
            "[EPOCH: 4,   100] loss: 0.953\n",
            "[EPOCH: 4,   200] loss: 0.955\n",
            "[EPOCH: 4,   300] loss: 0.913\n",
            "[EPOCH: 4,   400] loss: 0.881\n",
            "[EPOCH: 4,   500] loss: 0.862\n",
            "Accuracy on test set: 64 %\n",
            "[EPOCH: 5,   100] loss: 0.825\n",
            "[EPOCH: 5,   200] loss: 0.838\n",
            "[EPOCH: 5,   300] loss: 0.809\n",
            "[EPOCH: 5,   400] loss: 0.784\n",
            "[EPOCH: 5,   500] loss: 0.769\n",
            "Accuracy on test set: 69 %\n",
            "[EPOCH: 6,   100] loss: 0.700\n",
            "[EPOCH: 6,   200] loss: 0.689\n",
            "[EPOCH: 6,   300] loss: 0.691\n",
            "[EPOCH: 6,   400] loss: 0.673\n",
            "[EPOCH: 6,   500] loss: 0.675\n",
            "Accuracy on test set: 73 %\n",
            "[EPOCH: 7,   100] loss: 0.605\n",
            "[EPOCH: 7,   200] loss: 0.618\n",
            "[EPOCH: 7,   300] loss: 0.635\n",
            "[EPOCH: 7,   400] loss: 0.601\n",
            "[EPOCH: 7,   500] loss: 0.607\n",
            "Accuracy on test set: 76 %\n",
            "[EPOCH: 8,   100] loss: 0.545\n",
            "[EPOCH: 8,   200] loss: 0.549\n",
            "[EPOCH: 8,   300] loss: 0.560\n",
            "[EPOCH: 8,   400] loss: 0.550\n",
            "[EPOCH: 8,   500] loss: 0.557\n",
            "Accuracy on test set: 75 %\n",
            "[EPOCH: 9,   100] loss: 0.509\n",
            "[EPOCH: 9,   200] loss: 0.507\n",
            "[EPOCH: 9,   300] loss: 0.500\n",
            "[EPOCH: 9,   400] loss: 0.472\n",
            "[EPOCH: 9,   500] loss: 0.500\n",
            "Accuracy on test set: 78 %\n",
            "[EPOCH: 10,   100] loss: 0.442\n",
            "[EPOCH: 10,   200] loss: 0.473\n",
            "[EPOCH: 10,   300] loss: 0.469\n",
            "[EPOCH: 10,   400] loss: 0.459\n",
            "[EPOCH: 10,   500] loss: 0.464\n",
            "Accuracy on test set: 80 %\n",
            "[EPOCH: 11,   100] loss: 0.404\n",
            "[EPOCH: 11,   200] loss: 0.408\n",
            "[EPOCH: 11,   300] loss: 0.424\n",
            "[EPOCH: 11,   400] loss: 0.415\n",
            "[EPOCH: 11,   500] loss: 0.423\n",
            "Accuracy on test set: 78 %\n",
            "[EPOCH: 12,   100] loss: 0.376\n",
            "[EPOCH: 12,   200] loss: 0.382\n",
            "[EPOCH: 12,   300] loss: 0.393\n",
            "[EPOCH: 12,   400] loss: 0.400\n",
            "[EPOCH: 12,   500] loss: 0.378\n",
            "Accuracy on test set: 81 %\n",
            "[EPOCH: 13,   100] loss: 0.354\n",
            "[EPOCH: 13,   200] loss: 0.346\n",
            "[EPOCH: 13,   300] loss: 0.380\n",
            "[EPOCH: 13,   400] loss: 0.354\n",
            "[EPOCH: 13,   500] loss: 0.351\n",
            "Accuracy on test set: 82 %\n",
            "[EPOCH: 14,   100] loss: 0.326\n",
            "[EPOCH: 14,   200] loss: 0.336\n",
            "[EPOCH: 14,   300] loss: 0.350\n",
            "[EPOCH: 14,   400] loss: 0.339\n",
            "[EPOCH: 14,   500] loss: 0.331\n",
            "Accuracy on test set: 83 %\n",
            "[EPOCH: 15,   100] loss: 0.284\n",
            "[EPOCH: 15,   200] loss: 0.294\n",
            "[EPOCH: 15,   300] loss: 0.314\n",
            "[EPOCH: 15,   400] loss: 0.302\n",
            "[EPOCH: 15,   500] loss: 0.323\n",
            "Accuracy on test set: 81 %\n",
            "[EPOCH: 16,   100] loss: 0.270\n",
            "[EPOCH: 16,   200] loss: 0.316\n",
            "[EPOCH: 16,   300] loss: 0.287\n",
            "[EPOCH: 16,   400] loss: 0.287\n",
            "[EPOCH: 16,   500] loss: 0.302\n",
            "Accuracy on test set: 82 %\n",
            "[EPOCH: 17,   100] loss: 0.258\n",
            "[EPOCH: 17,   200] loss: 0.259\n",
            "[EPOCH: 17,   300] loss: 0.263\n",
            "[EPOCH: 17,   400] loss: 0.286\n",
            "[EPOCH: 17,   500] loss: 0.271\n",
            "Accuracy on test set: 83 %\n",
            "[EPOCH: 18,   100] loss: 0.241\n",
            "[EPOCH: 18,   200] loss: 0.232\n",
            "[EPOCH: 18,   300] loss: 0.254\n",
            "[EPOCH: 18,   400] loss: 0.258\n",
            "[EPOCH: 18,   500] loss: 0.255\n",
            "Accuracy on test set: 84 %\n",
            "[EPOCH: 19,   100] loss: 0.222\n",
            "[EPOCH: 19,   200] loss: 0.220\n",
            "[EPOCH: 19,   300] loss: 0.248\n",
            "[EPOCH: 19,   400] loss: 0.227\n",
            "[EPOCH: 19,   500] loss: 0.248\n",
            "Accuracy on test set: 82 %\n",
            "[EPOCH: 20,   100] loss: 0.203\n",
            "[EPOCH: 20,   200] loss: 0.222\n",
            "[EPOCH: 20,   300] loss: 0.224\n",
            "[EPOCH: 20,   400] loss: 0.212\n",
            "[EPOCH: 20,   500] loss: 0.237\n",
            "Accuracy on test set: 84 %\n",
            "[EPOCH: 21,   100] loss: 0.191\n",
            "[EPOCH: 21,   200] loss: 0.194\n",
            "[EPOCH: 21,   300] loss: 0.197\n",
            "[EPOCH: 21,   400] loss: 0.214\n",
            "[EPOCH: 21,   500] loss: 0.217\n",
            "Accuracy on test set: 83 %\n",
            "[EPOCH: 22,   100] loss: 0.189\n",
            "[EPOCH: 22,   200] loss: 0.185\n",
            "[EPOCH: 22,   300] loss: 0.202\n",
            "[EPOCH: 22,   400] loss: 0.199\n",
            "[EPOCH: 22,   500] loss: 0.197\n",
            "Accuracy on test set: 82 %\n",
            "[EPOCH: 23,   100] loss: 0.162\n",
            "[EPOCH: 23,   200] loss: 0.175\n",
            "[EPOCH: 23,   300] loss: 0.186\n",
            "[EPOCH: 23,   400] loss: 0.195\n",
            "[EPOCH: 23,   500] loss: 0.187\n",
            "Accuracy on test set: 84 %\n",
            "[EPOCH: 24,   100] loss: 0.155\n",
            "[EPOCH: 24,   200] loss: 0.163\n",
            "[EPOCH: 24,   300] loss: 0.155\n",
            "[EPOCH: 24,   400] loss: 0.170\n",
            "[EPOCH: 24,   500] loss: 0.190\n",
            "Accuracy on test set: 83 %\n",
            "[EPOCH: 25,   100] loss: 0.154\n",
            "[EPOCH: 25,   200] loss: 0.159\n",
            "[EPOCH: 25,   300] loss: 0.159\n",
            "[EPOCH: 25,   400] loss: 0.154\n",
            "[EPOCH: 25,   500] loss: 0.182\n",
            "Accuracy on test set: 84 %\n",
            "[EPOCH: 26,   100] loss: 0.144\n",
            "[EPOCH: 26,   200] loss: 0.151\n",
            "[EPOCH: 26,   300] loss: 0.150\n",
            "[EPOCH: 26,   400] loss: 0.157\n",
            "[EPOCH: 26,   500] loss: 0.164\n",
            "Accuracy on test set: 84 %\n",
            "[EPOCH: 27,   100] loss: 0.122\n",
            "[EPOCH: 27,   200] loss: 0.133\n",
            "[EPOCH: 27,   300] loss: 0.141\n",
            "[EPOCH: 27,   400] loss: 0.154\n",
            "[EPOCH: 27,   500] loss: 0.168\n",
            "Accuracy on test set: 84 %\n",
            "[EPOCH: 28,   100] loss: 0.133\n",
            "[EPOCH: 28,   200] loss: 0.135\n",
            "[EPOCH: 28,   300] loss: 0.132\n",
            "[EPOCH: 28,   400] loss: 0.140\n",
            "[EPOCH: 28,   500] loss: 0.133\n",
            "Accuracy on test set: 85 %\n",
            "[EPOCH: 29,   100] loss: 0.110\n",
            "[EPOCH: 29,   200] loss: 0.122\n",
            "[EPOCH: 29,   300] loss: 0.122\n",
            "[EPOCH: 29,   400] loss: 0.119\n",
            "[EPOCH: 29,   500] loss: 0.141\n",
            "Accuracy on test set: 84 %\n",
            "[EPOCH: 30,   100] loss: 0.107\n",
            "[EPOCH: 30,   200] loss: 0.125\n",
            "[EPOCH: 30,   300] loss: 0.129\n",
            "[EPOCH: 30,   400] loss: 0.126\n",
            "[EPOCH: 30,   500] loss: 0.127\n",
            "Accuracy on test set: 84 %\n",
            "[EPOCH: 31,   100] loss: 0.118\n",
            "[EPOCH: 31,   200] loss: 0.119\n",
            "[EPOCH: 31,   300] loss: 0.119\n",
            "[EPOCH: 31,   400] loss: 0.117\n",
            "[EPOCH: 31,   500] loss: 0.116\n",
            "Accuracy on test set: 85 %\n",
            "[EPOCH: 32,   100] loss: 0.105\n",
            "[EPOCH: 32,   200] loss: 0.114\n",
            "[EPOCH: 32,   300] loss: 0.106\n",
            "[EPOCH: 32,   400] loss: 0.122\n",
            "[EPOCH: 32,   500] loss: 0.113\n",
            "Accuracy on test set: 85 %\n",
            "[EPOCH: 33,   100] loss: 0.088\n",
            "[EPOCH: 33,   200] loss: 0.091\n",
            "[EPOCH: 33,   300] loss: 0.103\n",
            "[EPOCH: 33,   400] loss: 0.106\n",
            "[EPOCH: 33,   500] loss: 0.103\n",
            "Accuracy on test set: 84 %\n",
            "[EPOCH: 34,   100] loss: 0.089\n",
            "[EPOCH: 34,   200] loss: 0.091\n",
            "[EPOCH: 34,   300] loss: 0.102\n",
            "[EPOCH: 34,   400] loss: 0.097\n",
            "[EPOCH: 34,   500] loss: 0.107\n",
            "Accuracy on test set: 85 %\n",
            "[EPOCH: 35,   100] loss: 0.092\n",
            "[EPOCH: 35,   200] loss: 0.086\n",
            "[EPOCH: 35,   300] loss: 0.100\n",
            "[EPOCH: 35,   400] loss: 0.099\n",
            "[EPOCH: 35,   500] loss: 0.100\n",
            "Accuracy on test set: 85 %\n",
            "[EPOCH: 36,   100] loss: 0.072\n",
            "[EPOCH: 36,   200] loss: 0.080\n",
            "[EPOCH: 36,   300] loss: 0.085\n",
            "[EPOCH: 36,   400] loss: 0.085\n",
            "[EPOCH: 36,   500] loss: 0.093\n",
            "Accuracy on test set: 85 %\n",
            "[EPOCH: 37,   100] loss: 0.082\n",
            "[EPOCH: 37,   200] loss: 0.069\n",
            "[EPOCH: 37,   300] loss: 0.088\n",
            "[EPOCH: 37,   400] loss: 0.082\n",
            "[EPOCH: 37,   500] loss: 0.089\n",
            "Accuracy on test set: 85 %\n",
            "[EPOCH: 38,   100] loss: 0.068\n",
            "[EPOCH: 38,   200] loss: 0.076\n",
            "[EPOCH: 38,   300] loss: 0.082\n",
            "[EPOCH: 38,   400] loss: 0.073\n",
            "[EPOCH: 38,   500] loss: 0.082\n",
            "Accuracy on test set: 85 %\n",
            "[EPOCH: 39,   100] loss: 0.082\n",
            "[EPOCH: 39,   200] loss: 0.076\n",
            "[EPOCH: 39,   300] loss: 0.079\n",
            "[EPOCH: 39,   400] loss: 0.078\n",
            "[EPOCH: 39,   500] loss: 0.086\n",
            "Accuracy on test set: 84 %\n",
            "[EPOCH: 40,   100] loss: 0.074\n",
            "[EPOCH: 40,   200] loss: 0.073\n",
            "[EPOCH: 40,   300] loss: 0.069\n",
            "[EPOCH: 40,   400] loss: 0.082\n",
            "[EPOCH: 40,   500] loss: 0.082\n",
            "Accuracy on test set: 85 %\n",
            "[EPOCH: 41,   100] loss: 0.058\n",
            "[EPOCH: 41,   200] loss: 0.072\n",
            "[EPOCH: 41,   300] loss: 0.065\n",
            "[EPOCH: 41,   400] loss: 0.076\n",
            "[EPOCH: 41,   500] loss: 0.081\n",
            "Accuracy on test set: 84 %\n",
            "[EPOCH: 42,   100] loss: 0.064\n",
            "[EPOCH: 42,   200] loss: 0.065\n",
            "[EPOCH: 42,   300] loss: 0.067\n",
            "[EPOCH: 42,   400] loss: 0.069\n",
            "[EPOCH: 42,   500] loss: 0.080\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 43,   100] loss: 0.055\n",
            "[EPOCH: 43,   200] loss: 0.068\n",
            "[EPOCH: 43,   300] loss: 0.067\n",
            "[EPOCH: 43,   400] loss: 0.072\n",
            "[EPOCH: 43,   500] loss: 0.066\n",
            "Accuracy on test set: 85 %\n",
            "[EPOCH: 44,   100] loss: 0.049\n",
            "[EPOCH: 44,   200] loss: 0.067\n",
            "[EPOCH: 44,   300] loss: 0.057\n",
            "[EPOCH: 44,   400] loss: 0.062\n",
            "[EPOCH: 44,   500] loss: 0.063\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 45,   100] loss: 0.059\n",
            "[EPOCH: 45,   200] loss: 0.062\n",
            "[EPOCH: 45,   300] loss: 0.061\n",
            "[EPOCH: 45,   400] loss: 0.063\n",
            "[EPOCH: 45,   500] loss: 0.064\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 46,   100] loss: 0.062\n",
            "[EPOCH: 46,   200] loss: 0.049\n",
            "[EPOCH: 46,   300] loss: 0.051\n",
            "[EPOCH: 46,   400] loss: 0.059\n",
            "[EPOCH: 46,   500] loss: 0.055\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 47,   100] loss: 0.058\n",
            "[EPOCH: 47,   200] loss: 0.055\n",
            "[EPOCH: 47,   300] loss: 0.063\n",
            "[EPOCH: 47,   400] loss: 0.059\n",
            "[EPOCH: 47,   500] loss: 0.059\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 48,   100] loss: 0.049\n",
            "[EPOCH: 48,   200] loss: 0.051\n",
            "[EPOCH: 48,   300] loss: 0.053\n",
            "[EPOCH: 48,   400] loss: 0.057\n",
            "[EPOCH: 48,   500] loss: 0.054\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 49,   100] loss: 0.045\n",
            "[EPOCH: 49,   200] loss: 0.041\n",
            "[EPOCH: 49,   300] loss: 0.052\n",
            "[EPOCH: 49,   400] loss: 0.049\n",
            "[EPOCH: 49,   500] loss: 0.047\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 50,   100] loss: 0.044\n",
            "[EPOCH: 50,   200] loss: 0.043\n",
            "[EPOCH: 50,   300] loss: 0.051\n",
            "[EPOCH: 50,   400] loss: 0.057\n",
            "[EPOCH: 50,   500] loss: 0.055\n",
            "Accuracy on test set: 85 %\n",
            "[EPOCH: 51,   100] loss: 0.045\n",
            "[EPOCH: 51,   200] loss: 0.042\n",
            "[EPOCH: 51,   300] loss: 0.046\n",
            "[EPOCH: 51,   400] loss: 0.046\n",
            "[EPOCH: 51,   500] loss: 0.053\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 52,   100] loss: 0.045\n",
            "[EPOCH: 52,   200] loss: 0.048\n",
            "[EPOCH: 52,   300] loss: 0.041\n",
            "[EPOCH: 52,   400] loss: 0.045\n",
            "[EPOCH: 52,   500] loss: 0.048\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 53,   100] loss: 0.042\n",
            "[EPOCH: 53,   200] loss: 0.044\n",
            "[EPOCH: 53,   300] loss: 0.058\n",
            "[EPOCH: 53,   400] loss: 0.044\n",
            "[EPOCH: 53,   500] loss: 0.047\n",
            "Accuracy on test set: 84 %\n",
            "[EPOCH: 54,   100] loss: 0.036\n",
            "[EPOCH: 54,   200] loss: 0.041\n",
            "[EPOCH: 54,   300] loss: 0.038\n",
            "[EPOCH: 54,   400] loss: 0.043\n",
            "[EPOCH: 54,   500] loss: 0.046\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 55,   100] loss: 0.036\n",
            "[EPOCH: 55,   200] loss: 0.039\n",
            "[EPOCH: 55,   300] loss: 0.039\n",
            "[EPOCH: 55,   400] loss: 0.041\n",
            "[EPOCH: 55,   500] loss: 0.048\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 56,   100] loss: 0.037\n",
            "[EPOCH: 56,   200] loss: 0.040\n",
            "[EPOCH: 56,   300] loss: 0.048\n",
            "[EPOCH: 56,   400] loss: 0.039\n",
            "[EPOCH: 56,   500] loss: 0.043\n",
            "Accuracy on test set: 85 %\n",
            "[EPOCH: 57,   100] loss: 0.038\n",
            "[EPOCH: 57,   200] loss: 0.028\n",
            "[EPOCH: 57,   300] loss: 0.038\n",
            "[EPOCH: 57,   400] loss: 0.037\n",
            "[EPOCH: 57,   500] loss: 0.043\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 58,   100] loss: 0.031\n",
            "[EPOCH: 58,   200] loss: 0.033\n",
            "[EPOCH: 58,   300] loss: 0.042\n",
            "[EPOCH: 58,   400] loss: 0.041\n",
            "[EPOCH: 58,   500] loss: 0.037\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 59,   100] loss: 0.032\n",
            "[EPOCH: 59,   200] loss: 0.035\n",
            "[EPOCH: 59,   300] loss: 0.035\n",
            "[EPOCH: 59,   400] loss: 0.035\n",
            "[EPOCH: 59,   500] loss: 0.042\n",
            "Accuracy on test set: 85 %\n",
            "[EPOCH: 60,   100] loss: 0.031\n",
            "[EPOCH: 60,   200] loss: 0.031\n",
            "[EPOCH: 60,   300] loss: 0.039\n",
            "[EPOCH: 60,   400] loss: 0.039\n",
            "[EPOCH: 60,   500] loss: 0.034\n",
            "Accuracy on test set: 87 %\n",
            "[EPOCH: 61,   100] loss: 0.030\n",
            "[EPOCH: 61,   200] loss: 0.030\n",
            "[EPOCH: 61,   300] loss: 0.030\n",
            "[EPOCH: 61,   400] loss: 0.035\n",
            "[EPOCH: 61,   500] loss: 0.040\n",
            "Accuracy on test set: 87 %\n",
            "[EPOCH: 62,   100] loss: 0.031\n",
            "[EPOCH: 62,   200] loss: 0.030\n",
            "[EPOCH: 62,   300] loss: 0.030\n",
            "[EPOCH: 62,   400] loss: 0.026\n",
            "[EPOCH: 62,   500] loss: 0.036\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 63,   100] loss: 0.033\n",
            "[EPOCH: 63,   200] loss: 0.026\n",
            "[EPOCH: 63,   300] loss: 0.031\n",
            "[EPOCH: 63,   400] loss: 0.036\n",
            "[EPOCH: 63,   500] loss: 0.031\n",
            "Accuracy on test set: 85 %\n",
            "[EPOCH: 64,   100] loss: 0.025\n",
            "[EPOCH: 64,   200] loss: 0.028\n",
            "[EPOCH: 64,   300] loss: 0.034\n",
            "[EPOCH: 64,   400] loss: 0.034\n",
            "[EPOCH: 64,   500] loss: 0.033\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 65,   100] loss: 0.034\n",
            "[EPOCH: 65,   200] loss: 0.032\n",
            "[EPOCH: 65,   300] loss: 0.030\n",
            "[EPOCH: 65,   400] loss: 0.032\n",
            "[EPOCH: 65,   500] loss: 0.028\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 66,   100] loss: 0.025\n",
            "[EPOCH: 66,   200] loss: 0.028\n",
            "[EPOCH: 66,   300] loss: 0.037\n",
            "[EPOCH: 66,   400] loss: 0.036\n",
            "[EPOCH: 66,   500] loss: 0.038\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 67,   100] loss: 0.024\n",
            "[EPOCH: 67,   200] loss: 0.026\n",
            "[EPOCH: 67,   300] loss: 0.025\n",
            "[EPOCH: 67,   400] loss: 0.027\n",
            "[EPOCH: 67,   500] loss: 0.036\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 68,   100] loss: 0.033\n",
            "[EPOCH: 68,   200] loss: 0.026\n",
            "[EPOCH: 68,   300] loss: 0.022\n",
            "[EPOCH: 68,   400] loss: 0.034\n",
            "[EPOCH: 68,   500] loss: 0.031\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 69,   100] loss: 0.027\n",
            "[EPOCH: 69,   200] loss: 0.029\n",
            "[EPOCH: 69,   300] loss: 0.023\n",
            "[EPOCH: 69,   400] loss: 0.024\n",
            "[EPOCH: 69,   500] loss: 0.030\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 70,   100] loss: 0.030\n",
            "[EPOCH: 70,   200] loss: 0.025\n",
            "[EPOCH: 70,   300] loss: 0.022\n",
            "[EPOCH: 70,   400] loss: 0.028\n",
            "[EPOCH: 70,   500] loss: 0.033\n",
            "Accuracy on test set: 87 %\n",
            "[EPOCH: 71,   100] loss: 0.022\n",
            "[EPOCH: 71,   200] loss: 0.027\n",
            "[EPOCH: 71,   300] loss: 0.024\n",
            "[EPOCH: 71,   400] loss: 0.025\n",
            "[EPOCH: 71,   500] loss: 0.028\n",
            "Accuracy on test set: 85 %\n",
            "[EPOCH: 72,   100] loss: 0.022\n",
            "[EPOCH: 72,   200] loss: 0.028\n",
            "[EPOCH: 72,   300] loss: 0.021\n",
            "[EPOCH: 72,   400] loss: 0.017\n",
            "[EPOCH: 72,   500] loss: 0.032\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 73,   100] loss: 0.023\n",
            "[EPOCH: 73,   200] loss: 0.031\n",
            "[EPOCH: 73,   300] loss: 0.030\n",
            "[EPOCH: 73,   400] loss: 0.028\n",
            "[EPOCH: 73,   500] loss: 0.036\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 74,   100] loss: 0.029\n",
            "[EPOCH: 74,   200] loss: 0.029\n",
            "[EPOCH: 74,   300] loss: 0.021\n",
            "[EPOCH: 74,   400] loss: 0.021\n",
            "[EPOCH: 74,   500] loss: 0.024\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 75,   100] loss: 0.023\n",
            "[EPOCH: 75,   200] loss: 0.019\n",
            "[EPOCH: 75,   300] loss: 0.027\n",
            "[EPOCH: 75,   400] loss: 0.022\n",
            "[EPOCH: 75,   500] loss: 0.023\n",
            "Accuracy on test set: 87 %\n",
            "[EPOCH: 76,   100] loss: 0.022\n",
            "[EPOCH: 76,   200] loss: 0.020\n",
            "[EPOCH: 76,   300] loss: 0.024\n",
            "[EPOCH: 76,   400] loss: 0.026\n",
            "[EPOCH: 76,   500] loss: 0.023\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 77,   100] loss: 0.019\n",
            "[EPOCH: 77,   200] loss: 0.021\n",
            "[EPOCH: 77,   300] loss: 0.021\n",
            "[EPOCH: 77,   400] loss: 0.022\n",
            "[EPOCH: 77,   500] loss: 0.025\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 78,   100] loss: 0.018\n",
            "[EPOCH: 78,   200] loss: 0.020\n",
            "[EPOCH: 78,   300] loss: 0.018\n",
            "[EPOCH: 78,   400] loss: 0.020\n",
            "[EPOCH: 78,   500] loss: 0.020\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 79,   100] loss: 0.016\n",
            "[EPOCH: 79,   200] loss: 0.020\n",
            "[EPOCH: 79,   300] loss: 0.019\n",
            "[EPOCH: 79,   400] loss: 0.020\n",
            "[EPOCH: 79,   500] loss: 0.021\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 80,   100] loss: 0.016\n",
            "[EPOCH: 80,   200] loss: 0.017\n",
            "[EPOCH: 80,   300] loss: 0.018\n",
            "[EPOCH: 80,   400] loss: 0.020\n",
            "[EPOCH: 80,   500] loss: 0.019\n",
            "Accuracy on test set: 87 %\n",
            "[EPOCH: 81,   100] loss: 0.017\n",
            "[EPOCH: 81,   200] loss: 0.018\n",
            "[EPOCH: 81,   300] loss: 0.018\n",
            "[EPOCH: 81,   400] loss: 0.025\n",
            "[EPOCH: 81,   500] loss: 0.017\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 82,   100] loss: 0.017\n",
            "[EPOCH: 82,   200] loss: 0.024\n",
            "[EPOCH: 82,   300] loss: 0.023\n",
            "[EPOCH: 82,   400] loss: 0.024\n",
            "[EPOCH: 82,   500] loss: 0.022\n",
            "Accuracy on test set: 87 %\n",
            "[EPOCH: 83,   100] loss: 0.015\n",
            "[EPOCH: 83,   200] loss: 0.020\n",
            "[EPOCH: 83,   300] loss: 0.022\n",
            "[EPOCH: 83,   400] loss: 0.017\n",
            "[EPOCH: 83,   500] loss: 0.021\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 84,   100] loss: 0.018\n",
            "[EPOCH: 84,   200] loss: 0.016\n",
            "[EPOCH: 84,   300] loss: 0.021\n",
            "[EPOCH: 84,   400] loss: 0.015\n",
            "[EPOCH: 84,   500] loss: 0.016\n",
            "Accuracy on test set: 87 %\n",
            "[EPOCH: 85,   100] loss: 0.013\n",
            "[EPOCH: 85,   200] loss: 0.015\n",
            "[EPOCH: 85,   300] loss: 0.022\n",
            "[EPOCH: 85,   400] loss: 0.022\n",
            "[EPOCH: 85,   500] loss: 0.021\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 86,   100] loss: 0.016\n",
            "[EPOCH: 86,   200] loss: 0.020\n",
            "[EPOCH: 86,   300] loss: 0.019\n",
            "[EPOCH: 86,   400] loss: 0.022\n",
            "[EPOCH: 86,   500] loss: 0.029\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 87,   100] loss: 0.021\n",
            "[EPOCH: 87,   200] loss: 0.016\n",
            "[EPOCH: 87,   300] loss: 0.015\n",
            "[EPOCH: 87,   400] loss: 0.021\n",
            "[EPOCH: 87,   500] loss: 0.020\n",
            "Accuracy on test set: 87 %\n",
            "[EPOCH: 88,   100] loss: 0.016\n",
            "[EPOCH: 88,   200] loss: 0.014\n",
            "[EPOCH: 88,   300] loss: 0.018\n",
            "[EPOCH: 88,   400] loss: 0.013\n",
            "[EPOCH: 88,   500] loss: 0.016\n",
            "Accuracy on test set: 87 %\n",
            "[EPOCH: 89,   100] loss: 0.018\n",
            "[EPOCH: 89,   200] loss: 0.019\n",
            "[EPOCH: 89,   300] loss: 0.019\n",
            "[EPOCH: 89,   400] loss: 0.018\n",
            "[EPOCH: 89,   500] loss: 0.019\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 90,   100] loss: 0.012\n",
            "[EPOCH: 90,   200] loss: 0.018\n",
            "[EPOCH: 90,   300] loss: 0.013\n",
            "[EPOCH: 90,   400] loss: 0.018\n",
            "[EPOCH: 90,   500] loss: 0.021\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 91,   100] loss: 0.015\n",
            "[EPOCH: 91,   200] loss: 0.016\n",
            "[EPOCH: 91,   300] loss: 0.018\n",
            "[EPOCH: 91,   400] loss: 0.019\n",
            "[EPOCH: 91,   500] loss: 0.017\n",
            "Accuracy on test set: 87 %\n",
            "[EPOCH: 92,   100] loss: 0.015\n",
            "[EPOCH: 92,   200] loss: 0.018\n",
            "[EPOCH: 92,   300] loss: 0.020\n",
            "[EPOCH: 92,   400] loss: 0.019\n",
            "[EPOCH: 92,   500] loss: 0.015\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 93,   100] loss: 0.013\n",
            "[EPOCH: 93,   200] loss: 0.015\n",
            "[EPOCH: 93,   300] loss: 0.020\n",
            "[EPOCH: 93,   400] loss: 0.018\n",
            "[EPOCH: 93,   500] loss: 0.015\n",
            "Accuracy on test set: 87 %\n",
            "[EPOCH: 94,   100] loss: 0.015\n",
            "[EPOCH: 94,   200] loss: 0.015\n",
            "[EPOCH: 94,   300] loss: 0.017\n",
            "[EPOCH: 94,   400] loss: 0.014\n",
            "[EPOCH: 94,   500] loss: 0.014\n",
            "Accuracy on test set: 87 %\n",
            "[EPOCH: 95,   100] loss: 0.014\n",
            "[EPOCH: 95,   200] loss: 0.015\n",
            "[EPOCH: 95,   300] loss: 0.014\n",
            "[EPOCH: 95,   400] loss: 0.012\n",
            "[EPOCH: 95,   500] loss: 0.010\n",
            "Accuracy on test set: 87 %\n",
            "[EPOCH: 96,   100] loss: 0.013\n",
            "[EPOCH: 96,   200] loss: 0.013\n",
            "[EPOCH: 96,   300] loss: 0.013\n",
            "[EPOCH: 96,   400] loss: 0.014\n",
            "[EPOCH: 96,   500] loss: 0.016\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 97,   100] loss: 0.015\n",
            "[EPOCH: 97,   200] loss: 0.019\n",
            "[EPOCH: 97,   300] loss: 0.016\n",
            "[EPOCH: 97,   400] loss: 0.017\n",
            "[EPOCH: 97,   500] loss: 0.011\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 98,   100] loss: 0.012\n",
            "[EPOCH: 98,   200] loss: 0.018\n",
            "[EPOCH: 98,   300] loss: 0.019\n",
            "[EPOCH: 98,   400] loss: 0.016\n",
            "[EPOCH: 98,   500] loss: 0.015\n",
            "Accuracy on test set: 87 %\n",
            "[EPOCH: 99,   100] loss: 0.019\n",
            "[EPOCH: 99,   200] loss: 0.016\n",
            "[EPOCH: 99,   300] loss: 0.015\n",
            "[EPOCH: 99,   400] loss: 0.014\n",
            "[EPOCH: 99,   500] loss: 0.019\n",
            "Accuracy on test set: 86 %\n",
            "[EPOCH: 100,   100] loss: 0.012\n",
            "[EPOCH: 100,   200] loss: 0.014\n",
            "[EPOCH: 100,   300] loss: 0.022\n",
            "[EPOCH: 100,   400] loss: 0.019\n",
            "[EPOCH: 100,   500] loss: 0.014\n",
            "Accuracy on test set: 87 %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import resnet18\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "# Define your loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "#scheduler = StepLR(optimizer, step_size=50, gamma=0.1)\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "# Train your model\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # Get the inputs and labels from the data loader\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass, backward pass, and optimization\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        running_loss += loss.item()\n",
        "        train_acc += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "        if i % 100 == 99:\n",
        "            print('[EPOCH: %d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 100))\n",
        "            running_loss = 0.0\n",
        "            train_acc = 0.0\n",
        "\n",
        "    # Test your model\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            # Get the inputs and labels from the data loader\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass and prediction\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Compute accuracy\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy on test set: %d %%' % (100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "p16LWi-98T3J"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
